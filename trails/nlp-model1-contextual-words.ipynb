{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10107840,"sourceType":"datasetVersion","datasetId":6235330},{"sourceId":10184865,"sourceType":"datasetVersion","datasetId":6291877},{"sourceId":10256828,"sourceType":"datasetVersion","datasetId":6344926},{"sourceId":199085,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":169808,"modelId":192148}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" !pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:56:12.990331Z","iopub.execute_input":"2024-12-21T04:56:12.990601Z","iopub.status.idle":"2024-12-21T04:56:21.186356Z","shell.execute_reply.started":"2024-12-21T04:56:12.990567Z","shell.execute_reply":"2024-12-21T04:56:21.185178Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":" !pip install -U sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:28:18.127223Z","iopub.execute_input":"2024-12-21T04:28:18.127492Z","iopub.status.idle":"2024-12-21T04:28:26.784911Z","shell.execute_reply.started":"2024-12-21T04:28:18.127465Z","shell.execute_reply":"2024-12-21T04:28:26.784052Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport re\nfrom keras.models import Sequential\nfrom keras.layers import Input, Embedding,TimeDistributed, LSTM, Dropout, Bidirectional, Dense \nimport random\nfrom nltk.corpus import wordnet \nfrom keras.preprocessing import sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.utils import plot_model, to_categorical\nimport nltk\nnltk.download('punkt')  \nfrom nltk.corpus import wordnet \nfrom tqdm import tqdm\nimport nltk\nimport subprocess\nimport logging\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\n# Now you can import the NLTK resources as usual\nfrom nltk.corpus import wordnet\nnltk.download('punkt')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:56:26.773879Z","iopub.execute_input":"2024-12-21T04:56:26.774525Z","iopub.status.idle":"2024-12-21T04:56:26.811390Z","shell.execute_reply.started":"2024-12-21T04:56:26.774490Z","shell.execute_reply":"2024-12-21T04:56:26.810591Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /kaggle/working/...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /kaggle/working/corpora/wordnet.zip\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"replace /kaggle/working/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n(EOF or read error, treating as \"[N]one\" ...)\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":" def cosine(u, v):\n     return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:28:26.796969Z","iopub.status.idle":"2024-12-21T04:28:26.797245Z","shell.execute_reply.started":"2024-12-21T04:28:26.797104Z","shell.execute_reply":"2024-12-21T04:28:26.797119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" from sentence_transformers import SentenceTransformer\n\n embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:28:26.799041Z","iopub.status.idle":"2024-12-21T04:28:26.799482Z","shell.execute_reply.started":"2024-12-21T04:28:26.799264Z","shell.execute_reply":"2024-12-21T04:28:26.799287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" train_data = pd.read_json(\"/kaggle/input/pizza-dataset/PIZZA_train.json\", lines=True)\n train_data = train_data.sample(frac=0.01, random_state=42)\n src=train_data[\"train.SRC\"].to_numpy()\n top_decoupled=train_data[\"train.TOP-DECOUPLED\"].to_numpy()\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:28:26.801046Z","iopub.status.idle":"2024-12-21T04:28:26.801344Z","shell.execute_reply.started":"2024-12-21T04:28:26.801201Z","shell.execute_reply":"2024-12-21T04:28:26.801217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n logging.getLogger('sentence_transformers').setLevel(logging.ERROR)\n src_emd =[]\n count=0\n new_src=np.array([])\n new_top_decoupled=np.array([])\n for src_sent,top_sent in tqdm(zip(src,top_decoupled)):\n     flag=False\n     sent_embedding = embedding_model.encode(src_sent)\n     for emd in src_emd:\n         cos=cosine(emd,sent_embedding)\n         # print(cos)\n         if(cos>0.9):\n             flag=True\n             break\n     if(not flag):\n         count+=1\n         src_emd.append(sent_embedding)\n         new_src=np.append(new_src,src_sent)\n         new_top_decoupled=np.append(new_top_decoupled,top_sent)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:28:26.802650Z","iopub.status.idle":"2024-12-21T04:28:26.802930Z","shell.execute_reply.started":"2024-12-21T04:28:26.802787Z","shell.execute_reply":"2024-12-21T04:28:26.802806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprosses(src,top_decoupled):\n    word_tag=[]\n    for src,trg in  zip(src,top_decoupled):\n        out=extract_bio_tags(src,trg)\n        word_tag.append(out)\n    return word_tag","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:57:02.677403Z","iopub.execute_input":"2024-12-21T04:57:02.678115Z","iopub.status.idle":"2024-12-21T04:57:02.682474Z","shell.execute_reply.started":"2024-12-21T04:57:02.678078Z","shell.execute_reply":"2024-12-21T04:57:02.681483Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def get_pizza_order_synonyms(word):\n    \"\"\"\n    Get synonyms related to the 'pizza order' category.\n    \"\"\"\n    # Keywords related to pizza ordering\n    pizza_keywords = [\n        \"pizza\", \"cheese\", \"crust\", \"pepperoni\", \"mushroom\", \n        \"topping\", \"delivery\", \"order\",\"drink\",\"meat\",\"chicken\",\"size\",\"pizza style\",\"negation\"\n    ]\n    \n    synonyms = set()\n    for syn in wordnet.synsets(word):\n        # Check if the synset definition matches pizza-related keywords\n        if any(keyword in syn.definition().lower() for keyword in pizza_keywords):\n            for lemma in syn.lemmas():\n                synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n                synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n                synonyms.add(synonym)\n    \n    # Remove the input word from synonyms if it exists\n    if word in synonyms:\n        synonyms.remove(word)\n    \n    return list(synonyms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:28:26.806549Z","iopub.status.idle":"2024-12-21T04:28:26.806905Z","shell.execute_reply.started":"2024-12-21T04:28:26.806743Z","shell.execute_reply":"2024-12-21T04:28:26.806760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def synonym_replacement(sentence,top_decoupled, num_replacements=1):\n    words = sentence.split()  # Tokenize sentence\n    words_to_replace = [word for word in words if get_pizza_order_synonyms(word)]  # Only consider words with synonyms\n\n    # Shuffle the list to randomly select words for replacement\n    random.shuffle(words_to_replace)\n\n    num_replaced = 0\n    for word in words_to_replace:\n        if num_replaced >= num_replacements:\n            break\n        \n        synonyms = get_pizza_order_synonyms(word)\n        if synonyms:\n            synonym = random.choice(synonyms)  # Pick a random synonym\n            words = [synonym if w == word else w for w in words]\n            num_replaced += 1\n            top_decoupled=re.sub(word,synonym,top_decoupled)\n\n    return \" \".join(words),top_decoupled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:28:26.807948Z","iopub.status.idle":"2024-12-21T04:28:26.808370Z","shell.execute_reply.started":"2024-12-21T04:28:26.808142Z","shell.execute_reply":"2024-12-21T04:28:26.808165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"src=new_src\ntop_decoupled=new_top_decoupled\nsrc_augment=src\ntop_decoupled_augment=top_decoupled\nfor src_itr,top_itr in tqdm(zip(src_augment,top_decoupled_augment)):\n    srcc,topp=synonym_replacement(src_itr,top_itr,5)\n    src=np.append(src,srcc)\n    top_decoupled=np.append(top_decoupled,topp)\n   \n    \n\n\n\n    \n    \n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:28:26.809889Z","iopub.status.idle":"2024-12-21T04:28:26.810167Z","shell.execute_reply.started":"2024-12-21T04:28:26.810023Z","shell.execute_reply":"2024-12-21T04:28:26.810037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = \"/kaggle/input/dataset-65/src_65.txt\"\n\n# Read data into an array of strings\nwith open(file_path, \"r\") as file:\n    src = np.array([line.strip() for line in file])\n\nfile_path = \"/kaggle/input/dataset-65/top_decoupled_65.txt\"\n\n# Read data into an array of strings\nwith open(file_path, \"r\") as file:\n    top_decoupled = np.array([line.strip() for line in file])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:57:09.133352Z","iopub.execute_input":"2024-12-21T04:57:09.133728Z","iopub.status.idle":"2024-12-21T04:57:09.170922Z","shell.execute_reply.started":"2024-12-21T04:57:09.133693Z","shell.execute_reply":"2024-12-21T04:57:09.170250Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def extract_bio_tags(src, top_decoupled):\n    \n\n    # Parse the TOP-DECOUPLED structure\n    matches = re.findall(r'\\(NOT \\([^()]+\\) \\)|\\((?:[^()]+)\\)|\\(NOT \\(COMPLEX_TOPPING \\((?:.+?) \\) \\) \\)', top_decoupled)\n    ind = -1\n    if random.random() < 0.5:\n        ind = random.randint(0, len(src.split()) - 1)\n\n    # Map words to BIO tags\n    bio_tag_map = {}\n    for i, match in enumerate(matches):\n        match = re.sub(r'\\(|\\)', '', match)\n        match = match.strip()\n        words = match.split()  # Split multi-word values\n\n        if words[0] == \"NOT\":\n            if \"COMPLEX_TOPPING\" in words:\n                words.remove(\"COMPLEX_TOPPING\")\n                words.remove(\"NOT\")\n                indx = words.index(\"TOPPING\")\n                quantity = words[1:indx]\n                topping = words[indx + 1:]\n                for i, word in enumerate(quantity):\n                    if i == 0:\n                        bio_tag_map[word] = \"B-QUANTITY\"  # Begin tag\n                    else:\n                        bio_tag_map[word] = \"I-QUANTITY\"  # Inside tag\n                for i, word in enumerate(topping):\n                    if i == 0:\n                        bio_tag_map[word] = \"B-NOT-TOPPING\"  # Begin tag\n                    else:\n                        bio_tag_map[word] = \"I-NOT-TOPPING\"  # Inside tag\n                continue\n            else:\n                tag = words[0] + \"-\" + words[1]\n                words = words[2:]\n        else:\n            tag = words[0]\n            words = words[1:]\n\n        for i, word in enumerate(words):\n            if i == 0:\n                bio_tag_map[word] = f\"B-{tag}\"  # Begin tag\n            else:\n                bio_tag_map[word] = f\"I-{tag}\"  # Inside tag\n\n    src_words = src.split()\n    word_tag_pairs = []\n\n    for idx, word in enumerate(src_words):\n        # Get surrounding context words\n        context_before = src_words[idx - 1] if idx - 1 >= 0 else \"<START>\"\n        context_after = src_words[idx + 1] if idx + 1 < len(src_words) else \"<END>\"\n\n        # Add word, BIO tag, POS tag, and context\n        bio_tag = bio_tag_map.get(word, \"O\")\n        word_tag_pairs.append((word, bio_tag,  context_before, context_after))\n\n    if ind != -1:\n        word_tag_pairs[ind] = (\"<UNK>\", word_tag_pairs[ind][1], word_tag_pairs[ind][2], word_tag_pairs[ind][3])\n\n    return word_tag_pairs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:57:12.784835Z","iopub.execute_input":"2024-12-21T04:57:12.785765Z","iopub.status.idle":"2024-12-21T04:57:12.796946Z","shell.execute_reply.started":"2024-12-21T04:57:12.785712Z","shell.execute_reply":"2024-12-21T04:57:12.795975Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"sentences =preprosses(src,top_decoupled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:57:16.409096Z","iopub.execute_input":"2024-12-21T04:57:16.409758Z","iopub.status.idle":"2024-12-21T04:57:16.769390Z","shell.execute_reply.started":"2024-12-21T04:57:16.409725Z","shell.execute_reply":"2024-12-21T04:57:16.768454Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"\nmaxlen = max([len(s) for s in sentences])\nwords = [w[0] for sentence in sentences for w in sentence]\nwords = list(set(words))\nwords.append(\"<pad>\")\nwords.append(\"<START>\")\nwords.append(\"<END>\")\nn_words = len(words)\nprint('Number of unique words:', n_words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T05:00:20.646502Z","iopub.execute_input":"2024-12-21T05:00:20.647347Z","iopub.status.idle":"2024-12-21T05:00:20.670688Z","shell.execute_reply.started":"2024-12-21T05:00:20.647310Z","shell.execute_reply":"2024-12-21T05:00:20.669782Z"}},"outputs":[{"name":"stdout","text":"Number of unique words: 348\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"\n\ntags = [w[1] for sentence in sentences for w in sentence]\ntags=list(set(tags))\nprint(tags)\nn_tags = len(tags)\nprint('Number of unique Tags:', n_tags)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T05:00:22.919164Z","iopub.execute_input":"2024-12-21T05:00:22.919508Z","iopub.status.idle":"2024-12-21T05:00:22.941050Z","shell.execute_reply.started":"2024-12-21T05:00:22.919476Z","shell.execute_reply":"2024-12-21T05:00:22.940201Z"}},"outputs":[{"name":"stdout","text":"['B-VOLUME', 'I-NOT-TOPPING', 'I-QUANTITY', 'I-NOT-STYLE', 'B-CONTAINERTYPE', 'I-STYLE', 'O', 'B-STYLE', 'B-NOT-STYLE', 'I-NUMBER', 'I-SIZE', 'B-TOPPING', 'B-QUANTITY', 'I-TOPPING', 'B-NUMBER', 'I-DRINKTYPE', 'I-VOLUME', 'I-CONTAINERTYPE', 'B-DRINKTYPE', 'B-SIZE', 'B-NOT-TOPPING']\nNumber of unique Tags: 21\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"word2idx = {w: i for i, w in enumerate(words)}\ntag2idx = {t: i for i, t in enumerate(tags)}\nprint(tag2idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T05:00:25.350363Z","iopub.execute_input":"2024-12-21T05:00:25.351100Z","iopub.status.idle":"2024-12-21T05:00:25.355823Z","shell.execute_reply.started":"2024-12-21T05:00:25.351063Z","shell.execute_reply":"2024-12-21T05:00:25.355051Z"}},"outputs":[{"name":"stdout","text":"{'B-VOLUME': 0, 'I-NOT-TOPPING': 1, 'I-QUANTITY': 2, 'I-NOT-STYLE': 3, 'B-CONTAINERTYPE': 4, 'I-STYLE': 5, 'O': 6, 'B-STYLE': 7, 'B-NOT-STYLE': 8, 'I-NUMBER': 9, 'I-SIZE': 10, 'B-TOPPING': 11, 'B-QUANTITY': 12, 'I-TOPPING': 13, 'B-NUMBER': 14, 'I-DRINKTYPE': 15, 'I-VOLUME': 16, 'I-CONTAINERTYPE': 17, 'B-DRINKTYPE': 18, 'B-SIZE': 19, 'B-NOT-TOPPING': 20}\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"X_words = [[word2idx[w[0]] for w in s] for s in sentences]\nX_words = sequence.pad_sequences(maxlen=maxlen + 40, sequences=X_words, padding=\"post\", value=n_words - 1)\n\ny = [[tag2idx[w[1]] for w in s] for s in sentences]\ny = sequence.pad_sequences(maxlen=maxlen + 40, sequences=y, padding=\"post\", value=tag2idx['O'])\ny = np.array([to_categorical(i, num_classes=n_tags) for i in y])\n\n\nX_before = [[word2idx.get(w[2]) for w in s] for s in sentences]\nX_before = sequence.pad_sequences(maxlen=maxlen + 40, sequences=X_before, padding=\"post\", value=n_words - 1)\n\nX_after = [[word2idx.get(w[3]) for w in s] for s in sentences]\nX_after = sequence.pad_sequences(maxlen=maxlen + 40, sequences=X_after, padding=\"post\", value=n_words - 1)\n\n\n# Combining X_words and X_pos into a single feature input\nX_combined = np.concatenate([\n    np.expand_dims(X_words, axis=-1),  \n    np.expand_dims(X_before, axis=-1),  \n    np.expand_dims(X_after, axis=-1),  \n], axis=-1)\n\nprint('X_combined shape:', X_combined.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T05:01:12.730358Z","iopub.execute_input":"2024-12-21T05:01:12.731158Z","iopub.status.idle":"2024-12-21T05:01:13.339608Z","shell.execute_reply.started":"2024-12-21T05:01:12.731117Z","shell.execute_reply":"2024-12-21T05:01:13.338728Z"}},"outputs":[{"name":"stdout","text":"X_combined shape: (15704, 62, 3)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"class config():\n    VOCAB = n_words\n    MAX_LEN = maxlen+40\n    N_OUPUT = n_tags\n    \n    \n    EMBEDDING_VECTOR_LENGTH = 256\n    N_LSTM_CELLS = 150\n    RECURRENT_DROPOUT = 0.4\n    \n    OUTPUT_ACTIVATION = 'softmax'\n    \n    LOSS = 'categorical_crossentropy'\n    OPTIMIZER = 'adam'\n    METRICS = ['accuracy']\n    \n    MAX_EPOCHS = 5\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T05:01:26.383663Z","iopub.execute_input":"2024-12-21T05:01:26.384237Z","iopub.status.idle":"2024-12-21T05:01:26.388890Z","shell.execute_reply.started":"2024-12-21T05:01:26.384202Z","shell.execute_reply":"2024-12-21T05:01:26.387975Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\nfilepath = \"model.keras\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\nrlp = ReduceLROnPlateau(monitor='loss', patience=3, verbose=1)\n\nmodel = Sequential()\n# model.add(\n#     Embedding(\n#         input_dim=config.VOCAB, output_dim=config.EMBEDDING_VECTOR_LENGTH, input_length=config.MAX_LEN\n#     )\n# )\nmodel.add(\n    Bidirectional(\n        LSTM(config.N_LSTM_CELLS, return_sequences=True, recurrent_dropout=config.RECURRENT_DROPOUT)\n    )\n)\nmodel.add(\n    TimeDistributed(\n        Dense(config.N_OUPUT, activation=config.OUTPUT_ACTIVATION)\n    )\n)\nmodel.compile(loss=config.LOSS, optimizer=config.OPTIMIZER, metrics=config.METRICS)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T05:02:15.642806Z","iopub.execute_input":"2024-12-21T05:02:15.643394Z","iopub.status.idle":"2024-12-21T05:02:15.668489Z","shell.execute_reply.started":"2024-12-21T05:02:15.643361Z","shell.execute_reply":"2024-12-21T05:02:15.667873Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"history = model.fit(x=X_combined, y=y, validation_split=0.1,\n    callbacks=[es, ckpt, rlp], epochs=config.MAX_EPOCHS\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T05:02:18.319900Z","iopub.execute_input":"2024-12-21T05:02:18.320750Z","iopub.status.idle":"2024-12-21T05:06:49.723746Z","shell.execute_reply.started":"2024-12-21T05:02:18.320711Z","shell.execute_reply":"2024-12-21T05:06:49.723120Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.8816 - loss: 0.4754\nEpoch 1: loss improved from inf to 0.29292, saving model to model.keras\n\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 118ms/step - accuracy: 0.8817 - loss: 0.4750 - val_accuracy: 0.9458 - val_loss: 0.1768 - learning_rate: 0.0010\nEpoch 2/5\n\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9428 - loss: 0.1820\nEpoch 2: loss improved from 0.29292 to 0.17047, saving model to model.keras\n\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 120ms/step - accuracy: 0.9428 - loss: 0.1820 - val_accuracy: 0.9559 - val_loss: 0.1390 - learning_rate: 0.0010\nEpoch 3/5\n\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9535 - loss: 0.1466\nEpoch 3: loss improved from 0.17047 to 0.14081, saving model to model.keras\n\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 117ms/step - accuracy: 0.9535 - loss: 0.1466 - val_accuracy: 0.9645 - val_loss: 0.1141 - learning_rate: 0.0010\nEpoch 4/5\n\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9596 - loss: 0.1268\nEpoch 4: loss improved from 0.14081 to 0.12348, saving model to model.keras\n\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 120ms/step - accuracy: 0.9596 - loss: 0.1268 - val_accuracy: 0.9679 - val_loss: 0.1032 - learning_rate: 0.0010\nEpoch 5/5\n\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9630 - loss: 0.1152\nEpoch 5: loss improved from 0.12348 to 0.11232, saving model to model.keras\n\u001b[1m442/442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 119ms/step - accuracy: 0.9630 - loss: 0.1152 - val_accuracy: 0.9708 - val_loss: 0.0930 - learning_rate: 0.0010\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"dev_data = pd.read_json(\"/kaggle/input/pizza-dataset/PIZZA_dev.json\", lines=True)\nsrc_test=dev_data[\"dev.SRC\"].to_numpy()\ntop_test=dev_data[\"dev.TOP\"].to_numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T05:11:19.417274Z","iopub.execute_input":"2024-12-21T05:11:19.418151Z","iopub.status.idle":"2024-12-21T05:11:19.457964Z","shell.execute_reply.started":"2024-12-21T05:11:19.418109Z","shell.execute_reply":"2024-12-21T05:11:19.457159Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def evaluate(test_src, test_top):\n    true_len = 0\n    correct = 0\n\n    for src, top in zip(test_src, test_top):\n        src_words_original = src.split()\n        if len(src_words_original) > config.MAX_LEN:\n            continue\n\n        true_len += 1\n\n        # Replace unknown words with <UNK>\n        for word in src_words_original:\n            if word not in word2idx:\n                pattern = rf\"(?<=\\s){word}(?=\\s)\"\n                src = re.sub(pattern, \"<UNK>\", src)\n\n        # Prepare word-tag pairs with context for evaluation\n        gold_tags = extract_bio_tags(src, top)\n\n        # Convert words to indices\n        src_words = [w[0] for w in gold_tags]\n        src_indices = [word2idx.get(word, word2idx.get(\"<UNK>\")) for word in src_words]\n\n        # Prepare context indices\n        context_before_indices = [word2idx.get(w[2], word2idx.get(\"<START>\")) for w in gold_tags]\n        context_after_indices = [word2idx.get(w[3], word2idx.get(\"<END>\")) for w in gold_tags]\n\n        # Pad to max length\n        pad_token = word2idx.get(\"<pad>\")\n        src_indices = src_indices + [pad_token] * (config.MAX_LEN - len(src_indices))\n        context_before_indices = context_before_indices + [pad_token] * (config.MAX_LEN - len(context_before_indices))\n        context_after_indices = context_after_indices + [pad_token] * (config.MAX_LEN - len(context_after_indices))\n\n        # Combine word and context indices\n        src_combined = np.stack([\n            src_indices,\n            context_before_indices,\n            context_after_indices\n        ], axis=-1)\n        src_combined = np.expand_dims(src_combined, axis=0)  # Add batch dimension\n\n        # Get predictions from the model\n        predictions = model.predict(src_combined)\n        predictions = np.argmax(predictions, axis=-1)  # Convert to tag indices\n\n        # Map predictions back to tags\n        idx2tag = {v: k for k, v in tag2idx.items()}\n        predicted_tags = [idx2tag[i] for i in predictions[0][:len(src_words_original)]]\n\n        # Extract gold BIO tags for comparison\n        gold_bio_tags = [w[1] for w in gold_tags]\n\n        # Compare predictions to gold labels\n        if predicted_tags == gold_bio_tags:\n            correct += 1\n\n    print(f\"Accuracy: {correct / true_len * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T05:11:25.487436Z","iopub.execute_input":"2024-12-21T05:11:25.488255Z","iopub.status.idle":"2024-12-21T05:11:25.497738Z","shell.execute_reply.started":"2024-12-21T05:11:25.488219Z","shell.execute_reply":"2024-12-21T05:11:25.496874Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"evaluate(src_test,top_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport json\n\nwith open(\"word2idx_model1.json\", \"w\") as file:\n    json.dump(word2idx,file)\nwith open(\"tag2idx_model1.json\", \"w\") as file:\n    json.dump(tag2idx,file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:28:26.832656Z","iopub.status.idle":"2024-12-21T04:28:26.833063Z","shell.execute_reply.started":"2024-12-21T04:28:26.832849Z","shell.execute_reply":"2024-12-21T04:28:26.832872Z"}},"outputs":[],"execution_count":null}]}
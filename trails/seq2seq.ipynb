{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":10107840,"sourceType":"datasetVersion","datasetId":6235330}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:18:35.754637Z","iopub.execute_input":"2024-12-21T01:18:35.754914Z","iopub.status.idle":"2024-12-21T01:18:41.001735Z","shell.execute_reply.started":"2024-12-21T01:18:35.754878Z","shell.execute_reply":"2024-12-21T01:18:41.000686Z"}},"outputs":[{"name":"stdout","text":"Collecting nltk\n  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from nltk) (4.67.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk) (2024.11.6)\nInstalling collected packages: nltk\nSuccessfully installed nltk-3.9.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nimport nltk\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport tqdm\nimport torch.optim as optim\nimport torch.nn as nn\n# Download necessary NLTK resources if not already downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('punkt_tab')\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:18:41.003527Z","iopub.execute_input":"2024-12-21T01:18:41.003833Z","iopub.status.idle":"2024-12-21T01:19:17.120792Z","shell.execute_reply.started":"2024-12-21T01:18:41.003804Z","shell.execute_reply":"2024-12-21T01:19:17.119637Z"}},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1734743927.532995      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:479\nD1221 01:18:47.541259871      13 config.cc:196]                        gRPC EXPERIMENT call_status_override_on_cancellation   OFF (default:OFF)\nD1221 01:18:47.541273032      13 config.cc:196]                        gRPC EXPERIMENT call_v3                                OFF (default:OFF)\nD1221 01:18:47.541276364      13 config.cc:196]                        gRPC EXPERIMENT canary_client_privacy                  ON  (default:ON)\nD1221 01:18:47.541278761      13 config.cc:196]                        gRPC EXPERIMENT capture_base_context                   ON  (default:ON)\nD1221 01:18:47.541281148      13 config.cc:196]                        gRPC EXPERIMENT client_idleness                        ON  (default:ON)\nD1221 01:18:47.541283396      13 config.cc:196]                        gRPC EXPERIMENT client_privacy                         ON  (default:ON)\nD1221 01:18:47.541285673      13 config.cc:196]                        gRPC EXPERIMENT dapper_request_wire_size               OFF (default:OFF)\nD1221 01:18:47.541287858      13 config.cc:196]                        gRPC EXPERIMENT empty_experiment                       OFF (default:OFF)\nD1221 01:18:47.541290019      13 config.cc:196]                        gRPC EXPERIMENT event_engine_client                    OFF (default:OFF)\nD1221 01:18:47.541292138      13 config.cc:196]                        gRPC EXPERIMENT event_engine_dns                       ON  (default:ON)\nD1221 01:18:47.541294301      13 config.cc:196]                        gRPC EXPERIMENT event_engine_listener                  ON  (default:ON)\nD1221 01:18:47.541296464      13 config.cc:196]                        gRPC EXPERIMENT free_large_allocator                   OFF (default:OFF)\nD1221 01:18:47.541298663      13 config.cc:196]                        gRPC EXPERIMENT google_no_envelope_resolver            OFF (default:OFF)\nD1221 01:18:47.541300823      13 config.cc:196]                        gRPC EXPERIMENT http2_stats_fix                        OFF (default:OFF)\nD1221 01:18:47.541302986      13 config.cc:196]                        gRPC EXPERIMENT keepalive_fix                          OFF (default:OFF)\nD1221 01:18:47.541305138      13 config.cc:196]                        gRPC EXPERIMENT keepalive_server_fix                   ON  (default:ON)\nD1221 01:18:47.541307439      13 config.cc:196]                        gRPC EXPERIMENT loas_do_not_prefer_rekey_next_protocol OFF (default:OFF)\nD1221 01:18:47.541309659      13 config.cc:196]                        gRPC EXPERIMENT loas_prod_to_cloud_prefer_pfs_ciphers  OFF (default:OFF)\nD1221 01:18:47.541311861      13 config.cc:196]                        gRPC EXPERIMENT monitoring_experiment                  ON  (default:ON)\nD1221 01:18:47.541314081      13 config.cc:196]                        gRPC EXPERIMENT multiping                              OFF (default:OFF)\nD1221 01:18:47.541316267      13 config.cc:196]                        gRPC EXPERIMENT peer_state_based_framing               OFF (default:OFF)\nD1221 01:18:47.541318444      13 config.cc:196]                        gRPC EXPERIMENT pending_queue_cap                      ON  (default:ON)\nD1221 01:18:47.541320709      13 config.cc:196]                        gRPC EXPERIMENT pick_first_happy_eyeballs              ON  (default:ON)\nD1221 01:18:47.541322914      13 config.cc:196]                        gRPC EXPERIMENT promise_based_client_call              OFF (default:OFF)\nD1221 01:18:47.541344961      13 config.cc:196]                        gRPC EXPERIMENT promise_based_inproc_transport         OFF (default:OFF)\nD1221 01:18:47.541347549      13 config.cc:196]                        gRPC EXPERIMENT promise_based_server_call              OFF (default:OFF)\nD1221 01:18:47.541349823      13 config.cc:196]                        gRPC EXPERIMENT registered_method_lookup_in_transport  ON  (default:ON)\nD1221 01:18:47.541352054      13 config.cc:196]                        gRPC EXPERIMENT rfc_max_concurrent_streams             ON  (default:ON)\nD1221 01:18:47.541354366      13 config.cc:196]                        gRPC EXPERIMENT round_robin_delegate_to_pick_first     ON  (default:ON)\nD1221 01:18:47.541358049      13 config.cc:196]                        gRPC EXPERIMENT rstpit                                 OFF (default:OFF)\nD1221 01:18:47.541360699      13 config.cc:196]                        gRPC EXPERIMENT schedule_cancellation_over_write       OFF (default:OFF)\nD1221 01:18:47.541363048      13 config.cc:196]                        gRPC EXPERIMENT server_privacy                         ON  (default:ON)\nD1221 01:18:47.541365414      13 config.cc:196]                        gRPC EXPERIMENT tcp_frame_size_tuning                  OFF (default:OFF)\nD1221 01:18:47.541367619      13 config.cc:196]                        gRPC EXPERIMENT tcp_rcv_lowat                          OFF (default:OFF)\nD1221 01:18:47.541369783      13 config.cc:196]                        gRPC EXPERIMENT trace_record_callops                   OFF (default:OFF)\nD1221 01:18:47.541371966      13 config.cc:196]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size    OFF (default:OFF)\nD1221 01:18:47.541374089      13 config.cc:196]                        gRPC EXPERIMENT v3_backend_metric_filter               OFF (default:OFF)\nD1221 01:18:47.541376249      13 config.cc:196]                        gRPC EXPERIMENT v3_channel_idle_filters                ON  (default:ON)\nD1221 01:18:47.541378505      13 config.cc:196]                        gRPC EXPERIMENT v3_compression_filter                  ON  (default:ON)\nD1221 01:18:47.541380738      13 config.cc:196]                        gRPC EXPERIMENT v3_server_auth_filter                  OFF (default:OFF)\nD1221 01:18:47.541382917      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_clears_time_cache      OFF (default:OFF)\nD1221 01:18:47.541385042      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_dispatch               OFF (default:OFF)\nD1221 01:18:47.541387223      13 config.cc:196]                        gRPC EXPERIMENT write_size_cap                         ON  (default:ON)\nD1221 01:18:47.541389454      13 config.cc:196]                        gRPC EXPERIMENT write_size_policy                      ON  (default:ON)\nD1221 01:18:47.541391686      13 config.cc:196]                        gRPC EXPERIMENT wrr_delegate_to_pick_first             ON  (default:ON)\nI1221 01:18:47.541617546      13 ev_epoll1_linux.cc:123]               grpc epoll fd: 60\nD1221 01:18:47.541630475      13 ev_posix.cc:113]                      Using polling engine: epoll1\nD1221 01:18:47.552061025      13 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD1221 01:18:47.552070816      13 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD1221 01:18:47.552078843      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD1221 01:18:47.552082140      13 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD1221 01:18:47.552085448      13 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD1221 01:18:47.552088303      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin\"\nD1221 01:18:47.552120081      13 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD1221 01:18:47.552132130      13 dns_resolver_plugin.cc:43]            Using EventEngine dns resolver\nD1221 01:18:47.552148215      13 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD1221 01:18:47.552172372      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD1221 01:18:47.552179797      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD1221 01:18:47.552182931      13 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD1221 01:18:47.552187012      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD1221 01:18:47.552190185      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD1221 01:18:47.552193307      13 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD1221 01:18:47.552196488      13 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\nD1221 01:18:47.552227168      13 channel_init.cc:157]                  Filter server-auth not registered, but is referenced in the after clause of grpc-server-authz when building channel stack SERVER_CHANNEL\nI1221 01:18:47.554142391      13 ev_epoll1_linux.cc:359]               grpc epoll fd: 62\nI1221 01:18:47.574014873      13 tcp_socket_utils.cc:689]              Disabling AF_INET6 sockets because ::1 is not available.\nI1221 01:18:47.577870926     109 socket_utils_common_posix.cc:452]     Disabling AF_INET6 sockets because ::1 is not available.\nI1221 01:18:47.577924044     109 socket_utils_common_posix.cc:379]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE1221 01:18:47.583638992      13 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-12-21T01:18:47.583625481+00:00\", grpc_status:2}\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Initialize the tokenizer using NLTK\ntokenizer = word_tokenize\n\n# Custom stopwords\nstop_words = set(stopwords.words('english'))\nnegative_words = set([\"a\", \"an\", \"no\", \"not\", \"don't\", \"never\", \"none\", \"nothing\", \"neither\", \"nowhere\", \"nobody\", \"n't\"])\n\n# Remove negative words from stopwords\nstop_words -= negative_words\n\n# Preprocessing function to clean and tokenize sentences\ndef preprocess_order(order):\n    # Convert to lowercase\n    order = order.lower()\n    \n    # Remove punctuation\n    PUNCT_TO_REMOVE = string.punctuation\n    order = order.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n    \n    # Remove extra spaces\n    order = \" \".join(order.split())\n    \n    # Tokenize the text using the NLTK tokenizer\n    tokens = tokenizer(order)\n    \n    # Remove stopwords\n    filtered_tokens = [word for word in tokens if word not in stop_words]\n    \n    return filtered_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:19:17.121980Z","iopub.execute_input":"2024-12-21T01:19:17.122541Z","iopub.status.idle":"2024-12-21T01:19:17.130339Z","shell.execute_reply.started":"2024-12-21T01:19:17.122510Z","shell.execute_reply":"2024-12-21T01:19:17.129596Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"trainData = pd.read_json(\"/kaggle/input/PIZZA_train.json\", lines=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:19:17.131197Z","iopub.execute_input":"2024-12-21T01:19:17.131442Z","iopub.status.idle":"2024-12-21T01:19:40.858440Z","shell.execute_reply.started":"2024-12-21T01:19:17.131419Z","shell.execute_reply":"2024-12-21T01:19:40.857402Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"partialtrainData = trainData.sample(frac=0.05, random_state=42)\nsrc=partialtrainData[\"train.SRC\"].to_numpy()\ntrg=partialtrainData[\"train.EXR\"].to_numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:19:40.861036Z","iopub.execute_input":"2024-12-21T01:19:40.861374Z","iopub.status.idle":"2024-12-21T01:19:41.051384Z","shell.execute_reply.started":"2024-12-21T01:19:40.861347Z","shell.execute_reply":"2024-12-21T01:19:41.050370Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(src.shape)\nprint(trg.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:19:41.052534Z","iopub.execute_input":"2024-12-21T01:19:41.052808Z","iopub.status.idle":"2024-12-21T01:19:41.057075Z","shell.execute_reply.started":"2024-12-21T01:19:41.052775Z","shell.execute_reply":"2024-12-21T01:19:41.056355Z"}},"outputs":[{"name":"stdout","text":"(122822,)\n(122822,)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def save_data(input_sequences, target_sequences, input_filename, target_filename):\n    # Convert to tensors and save them\n    torch.save(input_sequences, input_filename)\n    torch.save(target_sequences, target_filename)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:19:41.058048Z","iopub.execute_input":"2024-12-21T01:19:41.058553Z","iopub.status.idle":"2024-12-21T01:19:41.067761Z","shell.execute_reply.started":"2024-12-21T01:19:41.058525Z","shell.execute_reply":"2024-12-21T01:19:41.067031Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Build vocab function (as described earlier)\ndef build_vocab(sentences):\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<unk>\", filters=\"\")\n    tokenizer.fit_on_texts(sentences)\n    \n    vocab = tokenizer.word_index\n    vocab[\"<pad>\"] = 0\n    vocab[\"<sos>\"] = len(vocab) \n    vocab[\"<eos>\"] = len(vocab) \n    \n    tokenizer.word_index = vocab\n    # print(tokenizer.word_index)\n    return tokenizer\n\n# Build vocabulary using the preprocessed orders (after tokenizing)\ninput_vocab = build_vocab([preprocess_order(order) for order in src])\noutput_vocab = build_vocab([preprocess_order(order) for order in trg])\n\n\n# Function to process data into numerical sequences\ndef process_data(sentences, vocab, is_output=False):\n    sequences = []\n    for sentence in sentences:\n        tokens = preprocess_order(sentence)  # Apply the preprocessing function\n        if is_output:\n            tokens = [\"<sos>\"] + tokens + [\"<eos>\"]  # Add <sos> and <eos> for output sequences\n        indices = [vocab[token] for token in tokens if token in vocab]  # Ensure token exists in vocab\n        sequences.append(indices)\n    \n    # Pad the sequences to make them uniform length\n    return tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post', value=vocab[\"<pad>\"])\n\n# Convert src_train and trg_train to numerical sequences\ninput_sequences = process_data(src, input_vocab.word_index)\noutput_sequences = process_data(trg, output_vocab.word_index, is_output=True)\n\n# Print the processed and padded sequences\nprint(input_sequences.shape)\nprint(output_sequences.shape)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:19:41.068663Z","iopub.execute_input":"2024-12-21T01:19:41.068891Z","iopub.status.idle":"2024-12-21T01:20:35.381755Z","shell.execute_reply.started":"2024-12-21T01:19:41.068865Z","shell.execute_reply":"2024-12-21T01:20:35.380777Z"}},"outputs":[{"name":"stdout","text":"(122822, 20)\n(122822, 32)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"vocab_size = len(input_vocab.word_index) \nprint(f\"input Vocabulary Size: {vocab_size}\")\nvocab_size = len(output_vocab.word_index) \nprint(f\"output Vocabulary Size: {vocab_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:20:35.382933Z","iopub.execute_input":"2024-12-21T01:20:35.383214Z","iopub.status.idle":"2024-12-21T01:20:35.387949Z","shell.execute_reply.started":"2024-12-21T01:20:35.383189Z","shell.execute_reply":"2024-12-21T01:20:35.387181Z"}},"outputs":[{"name":"stdout","text":"input Vocabulary Size: 295\noutput Vocabulary Size: 181\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Get the inverse mapping of the vocab (index -> word)\nindex_to_word = {index: word for word, index in input_vocab.word_index.items()}\n\n# Filter out padding (0) from the sequence and map indices to words\nwords_in_sequence = [index_to_word.get(index, \"<unk>\") for index in input_sequences[0] if index != input_vocab.word_index[\"<pad>\"]]\n\n\nprint(\"Words in input_sequences[0] without padding:\", words_in_sequence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:20:35.388911Z","iopub.execute_input":"2024-12-21T01:20:35.389141Z","iopub.status.idle":"2024-12-21T01:20:35.399899Z","shell.execute_reply.started":"2024-12-21T01:20:35.389119Z","shell.execute_reply":"2024-12-21T01:20:35.399144Z"}},"outputs":[{"name":"stdout","text":"Words in input_sequences[0] without padding: ['id', 'like', 'three', 'large', 'pies', 'pestos', 'yellow', 'peppers']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"input_sequences = torch.from_numpy(input_sequences)\noutput_sequences=torch.from_numpy(output_sequences)\n\n# Assuming `input_data` and `target_data` are your raw datasets\nsave_data(input_sequences, output_sequences, 'input_sequences.pt', 'target_sequences.pt')\n\ndel input_sequences\ndel output_sequences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:20:35.400833Z","iopub.execute_input":"2024-12-21T01:20:35.401066Z","iopub.status.idle":"2024-12-21T01:20:35.448117Z","shell.execute_reply.started":"2024-12-21T01:20:35.401044Z","shell.execute_reply":"2024-12-21T01:20:35.447349Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, input_file, target_file):\n        # Load the preprocessed data\n        self.input_data = torch.load(input_file)\n        self.target_data = torch.load(target_file)\n        \n    def __len__(self):\n        return len(self.input_data)\n    \n    def __getitem__(self, idx):\n        # Return a single sample (input, target) pair\n        input_sample = self.input_data[idx]\n        target_sample = self.target_data[idx]\n        return input_sample, target_sample\n\n# Create the Dataset object\ndataset = CustomDataset('input_sequences.pt', 'target_sequences.pt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:20:35.449006Z","iopub.execute_input":"2024-12-21T01:20:35.449231Z","iopub.status.idle":"2024-12-21T01:20:35.463619Z","shell.execute_reply.started":"2024-12-21T01:20:35.449209Z","shell.execute_reply":"2024-12-21T01:20:35.462670Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_13/3440725130.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.input_data = torch.load(input_file)\n/tmp/ipykernel_13/3440725130.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.target_data = torch.load(target_file)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_vocab_size, embed_size, hidden_size, num_layers):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        outputs, (hidden, cell) = self.lstm(embedded)\n        # Concatenate forward and backward hidden states\n        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1).unsqueeze(0)\n        cell = torch.cat((cell[-2], cell[-1]), dim=1).unsqueeze(0)\n        return outputs, hidden, cell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:20:35.464584Z","iopub.execute_input":"2024-12-21T01:20:35.464846Z","iopub.status.idle":"2024-12-21T01:20:35.470690Z","shell.execute_reply.started":"2024-12-21T01:20:35.464821Z","shell.execute_reply":"2024-12-21T01:20:35.469941Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, output_vocab_size, embed_size, hidden_size, num_layers):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_vocab_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(1)  # Add batch dimension\n        embedded = self.embedding(x)\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        predictions = self.fc(output.squeeze(1))\n        return predictions, hidden, cell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:20:35.473299Z","iopub.execute_input":"2024-12-21T01:20:35.473560Z","iopub.status.idle":"2024-12-21T01:20:35.481770Z","shell.execute_reply.started":"2024-12-21T01:20:35.473537Z","shell.execute_reply":"2024-12-21T01:20:35.481065Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = source.shape[0]\n        target_len = target.shape[1]\n        target_vocab_size = self.decoder.fc.out_features\n\n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n\n        # Encode source\n        _, hidden, cell = self.encoder(source)\n\n        # First input to decoder is <sos>\n        decoder_input = target[:, 0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n            outputs[:, t, :] = output\n            # Use teacher forcing\n            decoder_input = target[:, t] if torch.rand(1).item() < teacher_forcing_ratio else output.argmax(1)\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:20:35.482677Z","iopub.execute_input":"2024-12-21T01:20:35.482895Z","iopub.status.idle":"2024-12-21T01:20:35.495033Z","shell.execute_reply.started":"2024-12-21T01:20:35.482874Z","shell.execute_reply":"2024-12-21T01:20:35.494264Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"print(\"Input vocab size:\", len(input_vocab.word_index))\nprint(\"Output vocab size:\", len(output_vocab.word_index))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:20:35.496016Z","iopub.execute_input":"2024-12-21T01:20:35.496238Z","iopub.status.idle":"2024-12-21T01:20:35.504686Z","shell.execute_reply.started":"2024-12-21T01:20:35.496217Z","shell.execute_reply":"2024-12-21T01:20:35.503874Z"}},"outputs":[{"name":"stdout","text":"Input vocab size: 295\nOutput vocab size: 181\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"device = torch.device(\"cpu\")\n\n# Hyperparameters\ninput_vocab_size = len(input_vocab.word_index)\noutput_vocab_size = len(output_vocab.word_index)\nembed_size = 256\nhidden_size = 512\nnum_layers = 1\nlearning_rate = 0.001\n# batch_size = input_sequences.shape[0]\n# batch_size = 256\nepochs = 10\n\n# Create the DataLoader for batching\nbatch_size = 128\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\nclip=0.1\n\n# Initialize the model, optimizer, and loss function\nencoder = Encoder(input_vocab_size, embed_size, hidden_size, num_layers).to(device)\ndecoder = Decoder(output_vocab_size, embed_size, hidden_size * 2, num_layers).to(device)\nmodel = Seq2Seq(encoder, decoder, device).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss(ignore_index=output_vocab.word_index[\"<pad>\"])\n\n# Training loop\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss=0\n\n    for input_batch, target_batch in dataloader:\n        # Move data to the correct device\n        source = input_batch.to(device)\n        target = target_batch.to(device)\n        \n        optimizer.zero_grad()\n    \n        # Move data to device\n        # source = input_sequences.to(device)\n        # target = output_sequences.to(device)\n    \n        # Forward pass\n        outputs = model(source, target)\n    \n        # Reshape for calculating loss\n        outputs = outputs[:, 1:].reshape(-1, outputs.shape[2])\n        target = target[:, 1:].reshape(-1)\n    \n        target = target.long()\n        loss = criterion(outputs, target)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n        optimizer.step()\n        epoch_loss+=loss.item()\n\n    if(epoch_loss<=10**-4):\n        break\n    \n    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:48:16.413774Z","iopub.execute_input":"2024-12-21T01:48:16.414100Z","iopub.status.idle":"2024-12-21T03:03:23.572854Z","shell.execute_reply.started":"2024-12-21T01:48:16.414072Z","shell.execute_reply":"2024-12-21T03:03:23.571554Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 284.2593\nEpoch [2/10], Loss: 6.5323\nEpoch [3/10], Loss: 3.4658\nEpoch [4/10], Loss: 3.4314\nEpoch [5/10], Loss: 2.4000\nEpoch [6/10], Loss: 3.1380\nEpoch [7/10], Loss: 2.1699\nEpoch [8/10], Loss: 1.9265\nEpoch [9/10], Loss: 1.7594\nEpoch [10/10], Loss: 1.9494\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"devData = pd.read_json(\"/kaggle/input/PIZZA_dev.json\", lines=True)\nsrc_dev=devData[\"dev.SRC\"].to_numpy()\ntrg_dev=devData[\"dev.EXR\"].to_numpy()\n\ninput_dev_sequences = process_data(src_dev, input_vocab.word_index)\noutput_dev_sequences = process_data(trg_dev, output_vocab.word_index, is_output=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T03:13:03.278678Z","iopub.execute_input":"2024-12-21T03:13:03.279073Z","iopub.status.idle":"2024-12-21T03:13:03.379824Z","shell.execute_reply.started":"2024-12-21T03:13:03.279041Z","shell.execute_reply":"2024-12-21T03:13:03.378776Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\n\ndef evaluate(src, target):\n    total_correct = 0\n    model.eval()\n\n    # Forward pass through the model on the input data\n    with torch.no_grad():\n        for source, target_batch in zip(src, target):\n            # Convert numpy arrays to PyTorch tensors\n            source = torch.tensor(source, dtype=torch.long).to(device)\n            target_batch = torch.tensor(target_batch, dtype=torch.long).to(device)\n            \n            # # Debug shapes\n            # print(f\"source shape: {source.shape}\")\n            # print(f\"target_batch shape: {target_batch.shape}\")\n\n            # Ensure proper batch dimensions\n            if source.dim() == 1:\n                source = source.unsqueeze(0)  # Add batch dimension if missing\n            if target_batch.dim() == 1:\n                target_batch = target_batch.unsqueeze(0)\n\n            # Forward pass\n            output = model(source, target_batch)\n            \n            # Reshape for comparison\n            output = output[:, 1:].reshape(-1, output.shape[2])\n            target_batch = target_batch[:, 1:].reshape(-1)\n\n            # Mask padding tokens\n            mask = target_batch != 0\n            valid_output = output[mask]\n            valid_target = target_batch[mask]\n\n            # Get predicted tokens\n            _, predicted = torch.max(valid_output, 1)\n\n            # Calculate correct predictions\n            if torch.all(predicted == valid_target):\n                total_correct += 1\n\n    # Calculate overall accuracy\n    accuracy = total_correct / len(src)\n    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T03:17:11.436546Z","iopub.execute_input":"2024-12-21T03:17:11.436843Z","iopub.status.idle":"2024-12-21T03:17:11.445099Z","shell.execute_reply.started":"2024-12-21T03:17:11.436815Z","shell.execute_reply":"2024-12-21T03:17:11.444090Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"evaluate(input_dev_sequences,output_dev_sequences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T03:17:11.446104Z","iopub.execute_input":"2024-12-21T03:17:11.446385Z","iopub.status.idle":"2024-12-21T03:17:26.613129Z","shell.execute_reply.started":"2024-12-21T03:17:11.446358Z","shell.execute_reply":"2024-12-21T03:17:26.611904Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 6.03%\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# # Save the model's state dictionary\n# torch.save(model.state_dict(), \"seq2seq_state_dict.pth\")\n# print(\"Model state dictionary saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:20:36.044742Z","iopub.status.idle":"2024-12-21T01:20:36.045061Z","shell.execute_reply.started":"2024-12-21T01:20:36.044903Z","shell.execute_reply":"2024-12-21T01:20:36.044919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}